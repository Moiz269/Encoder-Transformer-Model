{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOZCAmuldgWp7/VHxY4J0Va",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moiz269/Encoder-Transformer-Model/blob/main/CompleteTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASKIMtXVKCVM"
      },
      "outputs": [],
      "source": [
        "# ATTENTION\n",
        "\n",
        "from math import sqrt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim=512, heads=8):\n",
        "\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim  # 512 by default\n",
        "        self.heads = heads  # 8 heads by default\n",
        "        self.head = int(self.embed_dim / self.heads)  # 512 / 8 = 64 by default\n",
        "        # note: The embedding dimension must be divided by the number of heads\n",
        "\n",
        "        # query, value, key: (64x64)\n",
        "        self.query = nn.Linear(self.head, self.head, bias=False)  # the Query metrix\n",
        "        self.value = nn.Linear(self.head, self.head, bias=False)  # the Value metrix\n",
        "        self.key = nn.Linear(self.head, self.head, bias=False)  # the Key metrix\n",
        "\n",
        "        # fully connected layer: 8*64x512 or 512x512\n",
        "        self.fc_out = nn.Linear(self.head * self.heads, embed_dim)\n",
        "\n",
        "    def forward(self, key, query, value, mask=None):\n",
        "        # Input of size: batch_size x sequence length x embedding dims\n",
        "        batch_size = key.size(0)\n",
        "        k_len, q_len, v_len = key.size(1), query.size(1), value.size(1)\n",
        "\n",
        "        # reshape from (batch_size x seq_len x embed_size) -> (batch_size x seq_len x heads x head)\n",
        "        # example: from (32x10x512) -> (32x10x8x64)\n",
        "        key = key.reshape(batch_size, k_len, self.heads, self.head)\n",
        "        query = query.reshape(batch_size, q_len, self.heads, self.head)\n",
        "        value = value.reshape(batch_size, v_len, self.heads, self.head)\n",
        "\n",
        "        key = self.key(key)  # (32x10x8x64)\n",
        "        query = self.query(query)  # (32x10x8x64)\n",
        "        value = self.value(value)  # (32x10x8x64)\n",
        "\n",
        "        ############### query x key ###############\n",
        "\n",
        "        # query shape: batch_size x q_len, heads, head, e.g: (32x10x8x64)\n",
        "        # key shape: batch_size x v_len, heads, head, e.g: (32x10x8x64)\n",
        "        # product shape should be: batch_size, heads, q_len, v_len, e.g: (32x8x10x10)\n",
        "        product = torch.einsum(\"bqhd,bkhd->bhqk\", [query, key])\n",
        "\n",
        "        # if mask (in decoder)\n",
        "        if mask is not None:\n",
        "            product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        product = product / sqrt(self.head)\n",
        "\n",
        "        scores = F.softmax(product, dim=-1)\n",
        "\n",
        "        ############### scores x value ###############\n",
        "\n",
        "        # scores shape: batch_size, heads, q_len, v_len, e.g: (32x8x10x10)\n",
        "        # value shape: batch_size, v_len, heads, head, e.g: (32x10x8x64)\n",
        "        # output: batch_size, heads, v_len, head, e.g: (32x10x512)\n",
        "\n",
        "        output = torch.einsum(\"nhql,nlhd->nqhd\", [scores, value]).reshape(\n",
        "            batch_size, q_len, self.heads * self.head\n",
        "        )\n",
        "\n",
        "        output = self.fc_out(output)  # (32x10x512) -> (32x10x512)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EMBEDDINGS\n",
        "\n",
        "from math import sin, cos, sqrt, log\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        output = self.embed(x) * sqrt(self.embed_dim)\n",
        "        # print(f\"Embedding shape: {output.shape}\")\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, max_seq_len=5000, dropout=0.1):\n",
        "\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        positional_encoding = torch.zeros(max_seq_len, self.embed_dim)\n",
        "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, embed_dim, 2) * -(log(10000.0) / embed_dim)\n",
        "        )\n",
        "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = positional_encoding.unsqueeze(0)\n",
        "\n",
        "        # we use register_buffer to save the \"pe\" parameter to the state_dict\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def pe_sin(self, position, i):\n",
        "        return sin(position / (10000 ** (2 * i) / self.embed_dim))\n",
        "\n",
        "    def pe_cos(self, position, i):\n",
        "        return cos(position / (10000 ** (2 * i) / self.embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "2HjaG-cAYSLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UTILS\n",
        "\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "\n",
        "def replicate(block, N=6) -> nn.ModuleList:\n",
        "\n",
        "    block_stack = nn.ModuleList([copy.deepcopy(block) for _ in range(N)])\n",
        "    return block_stack"
      ],
      "metadata": {
        "id": "sCT4jJwvgjdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ENCODER\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "def replicate(block, N=6) -> nn.ModuleList:\n",
        "\n",
        "    block_stack = nn.ModuleList([copy.deepcopy(block) for _ in range(N)])\n",
        "    return block_stack\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed_dim=512,\n",
        "                 heads=8,\n",
        "                 expansion_factor=4,\n",
        "                 dropout=0.2\n",
        "                 ):\n",
        "\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.attention = MultiHeadAttention(embed_dim, heads)  # the multi-head attention\n",
        "        self.norm = nn.LayerNorm(embed_dim)  # the normalization layer\n",
        "\n",
        "        # the FeedForward layer\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, expansion_factor * embed_dim),  # e.g: 512x(4*512) -> (512, 2048)\n",
        "            nn.ReLU(),  # ReLU activation function\n",
        "            nn.Linear(embed_dim * expansion_factor, embed_dim),  # e.g: 4*512)x512 -> (2048, 512)\n",
        "        )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, key, query, value, mask=None):\n",
        "        #################### Multi-Head Attention ####################\n",
        "        # first, pass the key, query and value through the multi head attention layer\n",
        "        attention_out = self.attention(key, query, value, mask)  # e.g.: 32x10x512\n",
        "\n",
        "        # then add the residual connection\n",
        "        attention_out = attention_out + value  # e.g.: 32x10x512\n",
        "\n",
        "        # after that we normalize and use dropout\n",
        "        attention_norm = self.dropout(self.norm(attention_out))  # e.g.: 32x10x512\n",
        "        # print(attention_norm.shape)\n",
        "\n",
        "        #################### Feed-Forwar Network ####################\n",
        "        fc_out = self.feed_forward(attention_norm)  # e.g.:32x10x512 -> #32x10x2048 -> 32x10x512\n",
        "\n",
        "        # Residual connection\n",
        "        fc_out = fc_out + attention_norm  # e.g.: 32x10x512\n",
        "\n",
        "        # dropout + normalization\n",
        "        fc_norm = self.dropout(self.norm(fc_out))  # e.g.: 32x10x512\n",
        "\n",
        "        return fc_norm\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 seq_len,\n",
        "                 vocab_size,\n",
        "                 embed_dim=512,\n",
        "                 num_blocks=6,\n",
        "                 expansion_factor=4,\n",
        "                 heads=8,\n",
        "                 dropout=0.2\n",
        "                 ):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # define the embedding: (vocabulary size x embedding dimension)\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # define the positional encoding: (embedding dimension x sequence length)\n",
        "        self.positional_encoder = PositionalEncoding(embed_dim, seq_len)\n",
        "\n",
        "        # define the set of blocks\n",
        "        # so we will have 'num_blocks' stacked on top of each other\n",
        "        self.blocks = replicate(TransformerBlock(embed_dim, heads, expansion_factor, dropout), num_blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.positional_encoder(self.embedding(x))\n",
        "        for block in self.blocks:\n",
        "            out = block(out, out, out)\n",
        "\n",
        "        # output shape: batch_size x seq_len x embed_size, e.g.: 32x10x512\n",
        "        return out"
      ],
      "metadata": {
        "id": "7ubxB-zaSLDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DECODER\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed_dim=512,\n",
        "                 heads=8,\n",
        "                 expansion_factor=4,\n",
        "                 dropout=0.2\n",
        "                 ):\n",
        "\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        # First define the Decoder Multi-head attention\n",
        "        self.attention = MultiHeadAttention(embed_dim, heads)\n",
        "        # normalization\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        # Dropout to avoid overfitting\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # finally th transformerBlock\n",
        "        self.transformerBlock = TransformerBlock(embed_dim, heads, expansion_factor, dropout)\n",
        "\n",
        "    def forward(self, key, query, x, mask):\n",
        "        # pass the inputs to the decoder multi-head attention\n",
        "        decoder_attention = self.attention(x, x, x, mask)\n",
        "        # residual connection + normalization\n",
        "        value = self.dropout(self.norm(decoder_attention + x))\n",
        "        # finally the transformerBlock (multi-head attention -> residual + norm -> feed forward -> residual + norm)\n",
        "        decoder_attention_output = self.transformerBlock(key, query, value)\n",
        "\n",
        "        return decoder_attention_output\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 target_vocab_size,\n",
        "                 seq_len,\n",
        "                 embed_dim=512,\n",
        "                 num_blocks=6,\n",
        "                 expansion_factor=4,\n",
        "                 heads=8,\n",
        "                 dropout=0.2\n",
        "                 ):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # define the embedding\n",
        "        self.embedding = nn.Embedding(target_vocab_size, embed_dim)\n",
        "        # the positional embedding\n",
        "        self.positional_encoder = PositionalEncoding(embed_dim, seq_len)\n",
        "\n",
        "        # define the set of decoders\n",
        "        self.blocks = replicate(DecoderBlock(embed_dim, heads, expansion_factor, dropout), num_blocks)\n",
        "        # dropout for overfitting\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, mask):\n",
        "        x = self.dropout(self.positional_encoder(self.embedding(x)))  # 32x10x512\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(encoder_output, x, encoder_output, mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "SyjazLBTSK8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMER\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed_dim,\n",
        "                 src_vocab_size,\n",
        "                 target_vocab_size,\n",
        "                 seq_len,\n",
        "                 num_blocks=6,\n",
        "                 expansion_factor=4,\n",
        "                 heads=8,\n",
        "                 dropout=0.2):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "\n",
        "        self.encoder = Encoder(seq_len=seq_len,\n",
        "                               vocab_size=src_vocab_size,\n",
        "                               embed_dim=embed_dim,\n",
        "                               num_blocks=num_blocks,\n",
        "                               expansion_factor=expansion_factor,\n",
        "                               heads=heads,\n",
        "                               dropout=dropout)\n",
        "\n",
        "        self.decoder = Decoder(target_vocab_size=target_vocab_size,\n",
        "                               seq_len=seq_len,\n",
        "                               embed_dim=embed_dim,\n",
        "                               num_blocks=num_blocks,\n",
        "                               expansion_factor=expansion_factor,\n",
        "                               heads=heads,\n",
        "                               dropout=dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        batch_size, trg_len = trg.shape\n",
        "        # returns the lower triangular part of matrix filled with ones\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            batch_size, 1, trg_len, trg_len\n",
        "        )\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        trg_mask = self.make_trg_mask(target)\n",
        "        enc_out = self.encoder(source)\n",
        "        outputs = self.decoder(target, enc_out, trg_mask)\n",
        "        output = F.softmax(self.fc_out(outputs), dim=-1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "PmV4LnSASKvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "#from utils import replicate\n",
        "#from attention import MultiHeadAttention\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "#from encoder import TransformerBlock\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, processors\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from tokenizers.normalizers import Sequence, Strip\n",
        "import pandas as pd\n",
        "import sys\n",
        "import csv\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dlSD0_LSmFj",
        "outputId": "e46af736-03fe-4de4-ba29-c5f25468ddca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        # self.text_data = text_data\n",
        "        #self.vocab = vocab\n",
        "        # self.tokenizer = tokenizer\n",
        "        self.code_snippets_df = pd.read_csv(csv_file)\n",
        "\n",
        "        # Convert the merged column into a list of strings\n",
        "        self.code_snippets = self.code_snippets_df['merged'].tolist()\n",
        "\n",
        "        self.tokenizer = self.prepare_tokenizer()\n",
        "\n",
        "    def prepare_tokenizer(self):\n",
        "        # Increase field size limit to handle large CSV fields\n",
        "        csv.field_size_limit(12456763)\n",
        "\n",
        "        # Initialize a tokenizer for Python code snippets\n",
        "        tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "\n",
        "        # Normalizer: Strip extra spaces (no lowercase since Python is case-sensitive)\n",
        "        tokenizer.normalizer = Sequence([Strip()])\n",
        "\n",
        "        # Pre-Tokenizer: Split based on whitespace and handle byte-level characters\n",
        "        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace()])\n",
        "\n",
        "        # Tokenizer Trainer: Train on the Python code snippets\n",
        "        trainer = trainers.WordPieceTrainer(\n",
        "            vocab_size=50000,\n",
        "            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        "            )\n",
        "\n",
        "        # Train the tokenizer on the merged code snippets\n",
        "        tokenizer.train_from_iterator(self.code_snippets, trainer=trainer)\n",
        "\n",
        "        # Post-processing: Adding [CLS] and [SEP] tokens for sequence processing\n",
        "        tokenizer.post_processor = TemplateProcessing(\n",
        "            single=\"[CLS] $A [SEP]\",\n",
        "            pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "            special_tokens=[\n",
        "                (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "                (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\"))\n",
        "            ]\n",
        "        )\n",
        "        tokenizer.enable_padding(length=MAX_SEQ_LENGTH)\n",
        "        tokenizer.enable_truncation(MAX_SEQ_LENGTH)\n",
        "        return tokenizer\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.code_snippets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.code_snippets[idx]\n",
        "        tokens = self.tokenizer.encode(text).tokens\n",
        "        token_ids = torch.tensor([self.tokenizer.get_vocab().get(token, \"[UNK]\") for token in tokens], dtype=torch.long)\n",
        "        # Return the input (src) and a shifted version as target (tgt)\n",
        "        return token_ids[:-1], token_ids[1:]  # src, tgt\n",
        "\n"
      ],
      "metadata": {
        "id": "c6Ef6iF2SmBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = (r'/content/modified_python_code_dataset.csv')\n",
        "\n",
        "# Define the path where the model is saved\n",
        "model_file = 'model.json'\n",
        "\n",
        "# Define TensorBoard log directory\n",
        "log_dir = \"runs/transformer_experiment\"\n",
        "\n",
        "# Create a TensorBoard SummaryWriter\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "# Check if the model file exists\n",
        "if os.path.exists(model_file):\n",
        "    print(\"Loading the pre-trained model...\")\n",
        "    model = torch.load(model_file)\n",
        "else:\n",
        "    print(\"Training the model from scratch...\")\n",
        "\n",
        "    EMBEDDING_SIZE = 512\n",
        "    NHEAD = 8\n",
        "    FFN_HID_DIM = 2048\n",
        "    NUM_DECODER_LAYERS = 6\n",
        "    MAX_SEQ_LENGTH = 100\n",
        "    VOCAB_SIZE = 50000\n",
        "\n",
        "    class DecoderBlock(nn.Module):\n",
        "        def __init__(self, embed_dim=512, heads=8, expansion_factor=4, dropout=0.2):\n",
        "            super(DecoderBlock, self).__init__()\n",
        "\n",
        "            # First define the Decoder Multi-head attention\n",
        "            self.attention = MultiHeadAttention(embed_dim, heads)\n",
        "            # normalization\n",
        "            self.norm = nn.LayerNorm(embed_dim)\n",
        "            # Dropout to avoid overfitting\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            # Finally the transformerBlock\n",
        "            self.transformerBlock = TransformerBlock(embed_dim, heads, expansion_factor, dropout)\n",
        "\n",
        "        def forward(self, key, query, x, mask):\n",
        "            # Pass the inputs to the decoder multi-head attention\n",
        "            decoder_attention = self.attention(x, x, x, mask)\n",
        "            # Residual connection + normalization\n",
        "            value = self.dropout(self.norm(decoder_attention + x))\n",
        "            # Return the value (output after attention and normalization)\n",
        "            return value\n",
        "\n",
        "    class Decoder(nn.Module):\n",
        "        def __init__(self, target_vocab_size, seq_len, embed_dim=512, num_blocks=6, expansion_factor=4, heads=8, dropout=0.2):\n",
        "            \"\"\"\n",
        "            The Decoder part of the Transformer architecture.\n",
        "\n",
        "            It is a set of stacked decoders on top of each other. In the paper, they used a stack of 6 decoders.\n",
        "            \"\"\"\n",
        "            super(Decoder, self).__init__()\n",
        "\n",
        "            # Define the embedding\n",
        "            self.embedding = nn.Embedding(target_vocab_size, embed_dim)\n",
        "            # The positional embedding\n",
        "            self.positional_encoder = PositionalEncoding(embed_dim, seq_len)\n",
        "            # Define the set of decoders\n",
        "            self.blocks = nn.ModuleList([DecoderBlock(embed_dim, heads, expansion_factor, dropout) for _ in range(num_blocks)])\n",
        "            # Dropout for overfitting\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        def forward(self, x, mask):\n",
        "            x = self.dropout(x)  # 32x10x512\n",
        "\n",
        "            for block in self.blocks:\n",
        "                x = block(x, x, x, mask)\n",
        "\n",
        "            return x\n",
        "\n",
        "    class PositionalEncoding(nn.Module):\n",
        "        def __init__(self, embedding_dim, max_seq_length):\n",
        "            super(PositionalEncoding, self).__init__()\n",
        "            pe = torch.zeros(max_seq_length, embedding_dim)\n",
        "            position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "            div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "            pe[:, 0::2] = torch.sin(position * div_term)\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "            pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "            self.register_buffer('pe', pe)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x + self.pe[:x.size(0), :]\n",
        "            return x\n",
        "\n",
        "    class TransformerModel(nn.Module):\n",
        "        def __init__(self, VOCAB_SIZE):\n",
        "            super(TransformerModel, self).__init__()\n",
        "            self.embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_SIZE)\n",
        "            self.pos_encoder = PositionalEncoding(EMBEDDING_SIZE, MAX_SEQ_LENGTH)\n",
        "            self.transformer_decoder = Decoder(VOCAB_SIZE, MAX_SEQ_LENGTH, EMBEDDING_SIZE, heads=NHEAD)\n",
        "            self.fc_out = nn.Linear(EMBEDDING_SIZE, VOCAB_SIZE)\n",
        "            self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "        def generate_square_subsequent_mask(self, sz):\n",
        "            mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "            return mask\n",
        "\n",
        "        def forward(self, src):\n",
        "            src_mask = self.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
        "            src = self.embedding(src) * math.sqrt(EMBEDDING_SIZE)\n",
        "            src = self.pos_encoder(src)\n",
        "            output = self.transformer_decoder(src, src_mask)\n",
        "            output = self.fc_out(output)\n",
        "            output = self.softmax(output)\n",
        "            return output\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "   device = torch.device(f'cuda:{torch.cuda.current_device()}')\n",
        "        #device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnYDhJYvSl6K",
        "outputId": "76dd0028-3981-4e56-e1ff-06e136cb5339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model from scratch...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Model definition\n",
        "model = TransformerModel(VOCAB_SIZE).to(device)\n",
        "\n",
        "    # DATA SPLITTING\n",
        "\n",
        "dataset = CustomTextDataset(csv_file)\n",
        "train_ratio=0.7\n",
        "val_ratio=0.1\n",
        "test_ratio=0.2\n",
        "\n",
        "# Split dataset into train and temp (temp= train + test)\n",
        "\n",
        "train_size=int(train_ratio*len(dataset))\n",
        "temp_size=len(dataset)-train_size\n",
        "train_dataset, temp_dataset=random_split(dataset,[train_size, temp_size])\n",
        "\n",
        "# Split temp data into validation and test sets\n",
        "val_size=int(val_ratio/(val_ratio+test_ratio)*temp_size)\n",
        "test_size=temp_size-val_size\n",
        "val_dataset, test_dataset=random_split(temp_dataset,[val_size,test_size])\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# TRAINING LOOP\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "dataset = CustomTextDataset(csv_file)  # Initialize the dataset\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "#dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{NUM_EPOCHS}', leave=True)\n",
        "\n",
        "        for i, (src, tgt) in progress_bar:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(src)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(output.view(-1, VOCAB_SIZE), tgt.reshape(-1))\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update model parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate the loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Calculate average loss and perplexity\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        train_perplexity = math.exp(avg_train_loss)\n",
        "\n",
        "        # Log to TensorBoard\n",
        "        writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch+1)\n",
        "        writer.add_scalar(\"Perplexity/Train\", train_perplexity, epoch+1)\n",
        "\n",
        "        # VALIDATION LOOP\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss=0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for src, tgt in val_loader:\n",
        "                src, tgt = src.to(device), tgt.to(device)\n",
        "                output=model(src)\n",
        "                val_loss=criterion(output.view(-1, VOCAB_SIZE), tgt.reshape(-1))\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "        # Calculate avg loss and perplexity to TB\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_perplexity = math.exp(avg_val_loss)\n",
        "\n",
        "        # Log validation loss and perplexity to TensorBoard\n",
        "        writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch+1)\n",
        "        writer.add_scalar(\"Perplexity/Validation\", val_perplexity, epoch+1)\n",
        "\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {avg_train_loss:.4f}, Train Perplexity: {train_perplexity:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Perplexity: {val_perplexity:.4f}')\n",
        "\n",
        "# Save the trained model to a file\n",
        "torch.save(model, model_file)\n",
        "\n",
        "# Close TensorBoard writer\n",
        "writer.close()\n",
        "\n",
        "# Load the model from the saved file\n",
        "# model = model.from_file(\"model.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmSUXf64dKnp",
        "outputId": "b3f9d1c1-6395-410a-a254-b1084ef64bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50:   1%|          | 238/24479 [2:00:39<197:36:48, 29.35s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# print(torch.cuda.is_available())\n",
        "# print(torch.cuda.current_device())\n",
        "# device=torch.device('cuda')\n",
        "# device"
      ],
      "metadata": {
        "id": "p-iwhBwio038"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}