{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7OAWgFXPbdr4"
      },
      "outputs": [],
      "source": [
        "# ATTENTION\n",
        "\n",
        "from math import sqrt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim=512, heads=8):\n",
        "\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim  # 512 by default\n",
        "        self.heads = heads  # 8 heads by default\n",
        "        self.head = int(self.embed_dim / self.heads)  # 512 / 8 = 64 by default\n",
        "        # note: The embedding dimension must be divided by the number of heads\n",
        "\n",
        "        # query, value, key: (64x64)\n",
        "        self.query = nn.Linear(self.head, self.head, bias=False)  # the Query metrix\n",
        "        self.value = nn.Linear(self.head, self.head, bias=False)  # the Value metrix\n",
        "        self.key = nn.Linear(self.head, self.head, bias=False)  # the Key metrix\n",
        "\n",
        "        # fully connected layer: 8*64x512 or 512x512\n",
        "        self.fc_out = nn.Linear(self.head * self.heads, embed_dim)\n",
        "\n",
        "    def forward(self, key, query, value, mask=None):\n",
        "        # Input of size: batch_size x sequence length x embedding dims\n",
        "        batch_size = key.size(0)\n",
        "        k_len, q_len, v_len = key.size(1), query.size(1), value.size(1)\n",
        "\n",
        "        # reshape from (batch_size x seq_len x embed_size) -> (batch_size x seq_len x heads x head)\n",
        "        # example: from (32x10x512) -> (32x10x8x64)\n",
        "        key = key.reshape(batch_size, k_len, self.heads, self.head)\n",
        "        query = query.reshape(batch_size, q_len, self.heads, self.head)\n",
        "        value = value.reshape(batch_size, v_len, self.heads, self.head)\n",
        "\n",
        "        key = self.key(key)  # (32x10x8x64)\n",
        "        query = self.query(query)  # (32x10x8x64)\n",
        "        value = self.value(value)  # (32x10x8x64)\n",
        "\n",
        "        ############### query x key ###############\n",
        "\n",
        "        # query shape: batch_size x q_len, heads, head, e.g: (32x10x8x64)\n",
        "        # key shape: batch_size x v_len, heads, head, e.g: (32x10x8x64)\n",
        "        # product shape should be: batch_size, heads, q_len, v_len, e.g: (32x8x10x10)\n",
        "        product = torch.einsum(\"bqhd,bkhd->bhqk\", [query, key])\n",
        "\n",
        "        # if mask (in decoder)\n",
        "        if mask is not None:\n",
        "            product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        product = product / sqrt(self.head)\n",
        "\n",
        "        scores = F.softmax(product, dim=-1)\n",
        "\n",
        "        ############### scores x value ###############\n",
        "\n",
        "        # scores shape: batch_size, heads, q_len, v_len, e.g: (32x8x10x10)\n",
        "        # value shape: batch_size, v_len, heads, head, e.g: (32x10x8x64)\n",
        "        # output: batch_size, heads, v_len, head, e.g: (32x10x512)\n",
        "\n",
        "        output = torch.einsum(\"nhql,nlhd->nqhd\", [scores, value]).reshape(\n",
        "            batch_size, q_len, self.heads * self.head\n",
        "        )\n",
        "\n",
        "        output = self.fc_out(output)  # (32x10x512) -> (32x10x512)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DPfeOZpDeepa"
      },
      "outputs": [],
      "source": [
        "# EMBEDDINGS\n",
        "\n",
        "from math import sin, cos, sqrt, log\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        output = self.embed(x) * sqrt(self.embed_dim)\n",
        "        # print(f\"Embedding shape: {output.shape}\")\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, max_seq_len=5000, dropout=0.1):\n",
        "\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        positional_encoding = torch.zeros(max_seq_len, self.embed_dim)\n",
        "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, embed_dim, 2) * -(log(10000.0) / embed_dim)\n",
        "        )\n",
        "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = positional_encoding.unsqueeze(0)\n",
        "\n",
        "        # we use register_buffer to save the \"pe\" parameter to the state_dict\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def pe_sin(self, position, i):\n",
        "        return sin(position / (10000 ** (2 * i) / self.embed_dim))\n",
        "\n",
        "    def pe_cos(self, position, i):\n",
        "        return cos(position / (10000 ** (2 * i) / self.embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UsNbk7Q3eLkb"
      },
      "outputs": [],
      "source": [
        "# UTILS\n",
        "\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "\n",
        "def replicate(block, N=6) -> nn.ModuleList:\n",
        "\n",
        "    block_stack = nn.ModuleList([copy.deepcopy(block) for _ in range(N)])\n",
        "    return block_stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x6PCnvNzekBp"
      },
      "outputs": [],
      "source": [
        "# ENCODER\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "def replicate(block, N=6) -> nn.ModuleList:\n",
        "\n",
        "    block_stack = nn.ModuleList([copy.deepcopy(block) for _ in range(N)])\n",
        "    return block_stack\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed_dim=512,\n",
        "                 heads=8,\n",
        "                 expansion_factor=4,\n",
        "                 dropout=0.2\n",
        "                 ):\n",
        "\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.attention = MultiHeadAttention(embed_dim, heads)  # the multi-head attention\n",
        "        self.norm = nn.LayerNorm(embed_dim)  # the normalization layer\n",
        "\n",
        "        # the FeedForward layer\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, expansion_factor * embed_dim),  # e.g: 512x(4*512) -> (512, 2048)\n",
        "            nn.ReLU(),  # ReLU activation function\n",
        "            nn.Linear(embed_dim * expansion_factor, embed_dim),  # e.g: 4*512)x512 -> (2048, 512)\n",
        "        )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, key, query, value, mask=None):\n",
        "        #################### Multi-Head Attention ####################\n",
        "        # first, pass the key, query and value through the multi head attention layer\n",
        "        attention_out = self.attention(key, query, value, mask)  # e.g.: 32x10x512\n",
        "\n",
        "        # then add the residual connection\n",
        "        attention_out = attention_out + value  # e.g.: 32x10x512\n",
        "\n",
        "        # after that we normalize and use dropout\n",
        "        attention_norm = self.dropout(self.norm(attention_out))  # e.g.: 32x10x512\n",
        "        # print(attention_norm.shape)\n",
        "\n",
        "        #################### Feed-Forwar Network ####################\n",
        "        fc_out = self.feed_forward(attention_norm)  # e.g.:32x10x512 -> #32x10x2048 -> 32x10x512\n",
        "\n",
        "        # Residual connection\n",
        "        fc_out = fc_out + attention_norm  # e.g.: 32x10x512\n",
        "\n",
        "        # dropout + normalization\n",
        "        fc_norm = self.dropout(self.norm(fc_out))  # e.g.: 32x10x512\n",
        "\n",
        "        return fc_norm\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 seq_len,\n",
        "                 vocab_size,\n",
        "                 embed_dim=512,\n",
        "                 num_blocks=6,\n",
        "                 expansion_factor=4,\n",
        "                 heads=8,\n",
        "                 dropout=0.2\n",
        "                 ):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # define the embedding: (vocabulary size x embedding dimension)\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # define the positional encoding: (embedding dimension x sequence length)\n",
        "        self.positional_encoder = PositionalEncoding(embed_dim, seq_len)\n",
        "\n",
        "        # define the set of blocks\n",
        "        # so we will have 'num_blocks' stacked on top of each other\n",
        "        self.blocks = replicate(TransformerBlock(embed_dim, heads, expansion_factor, dropout), num_blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.positional_encoder(self.embedding(x))\n",
        "        for block in self.blocks:\n",
        "            out = block(out, out, out)\n",
        "\n",
        "        # output shape: batch_size x seq_len x embed_size, e.g.: 32x10x512\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "csAIDhEFcEKr"
      },
      "outputs": [],
      "source": [
        "# DECODER\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed_dim=512,\n",
        "                 heads=8,\n",
        "                 expansion_factor=4,\n",
        "                 dropout=0.2\n",
        "                 ):\n",
        "\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        # First define the Decoder Multi-head attention\n",
        "        self.attention = MultiHeadAttention(embed_dim, heads)\n",
        "        # normalization\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        # Dropout to avoid overfitting\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # finally th transformerBlock\n",
        "        self.transformerBlock = TransformerBlock(embed_dim, heads, expansion_factor, dropout)\n",
        "\n",
        "    def forward(self, key, query, x, mask):\n",
        "        # pass the inputs to the decoder multi-head attention\n",
        "        decoder_attention = self.attention(x, x, x, mask)\n",
        "        # residual connection + normalization\n",
        "        value = self.dropout(self.norm(decoder_attention + x))\n",
        "        # finally the transformerBlock (multi-head attention -> residual + norm -> feed forward -> residual + norm)\n",
        "        decoder_attention_output = self.transformerBlock(key, query, value)\n",
        "\n",
        "        return decoder_attention_output\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 target_vocab_size,\n",
        "                 seq_len,\n",
        "                 embed_dim=512,\n",
        "                 num_blocks=6,\n",
        "                 expansion_factor=4,\n",
        "                 heads=8,\n",
        "                 dropout=0.2\n",
        "                 ):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # define the embedding\n",
        "        self.embedding = nn.Embedding(target_vocab_size, embed_dim)\n",
        "        # the positional embedding\n",
        "        self.positional_encoder = PositionalEncoding(embed_dim, seq_len)\n",
        "\n",
        "        # define the set of decoders\n",
        "        self.blocks = replicate(DecoderBlock(embed_dim, heads, expansion_factor, dropout), num_blocks)\n",
        "        # dropout for overfitting\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, mask):\n",
        "        x = self.dropout(self.positional_encoder(self.embedding(x)))  # 32x10x512\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(encoder_output, x, encoder_output, mask)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ajuda9CthmDh"
      },
      "outputs": [],
      "source": [
        "# TRANSFORMER\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed_dim,\n",
        "                 src_vocab_size,\n",
        "                 target_vocab_size,\n",
        "                 seq_len,\n",
        "                 num_blocks=6,\n",
        "                 expansion_factor=4,\n",
        "                 heads=8,\n",
        "                 dropout=0.2):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "\n",
        "        self.encoder = Encoder(seq_len=seq_len,\n",
        "                               vocab_size=src_vocab_size,\n",
        "                               embed_dim=embed_dim,\n",
        "                               num_blocks=num_blocks,\n",
        "                               expansion_factor=expansion_factor,\n",
        "                               heads=heads,\n",
        "                               dropout=dropout)\n",
        "\n",
        "        self.decoder = Decoder(target_vocab_size=target_vocab_size,\n",
        "                               seq_len=seq_len,\n",
        "                               embed_dim=embed_dim,\n",
        "                               num_blocks=num_blocks,\n",
        "                               expansion_factor=expansion_factor,\n",
        "                               heads=heads,\n",
        "                               dropout=dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        batch_size, trg_len = trg.shape\n",
        "        # returns the lower triangular part of matrix filled with ones\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            batch_size, 1, trg_len, trg_len\n",
        "        )\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        trg_mask = self.make_trg_mask(target)\n",
        "        enc_out = self.encoder(source)\n",
        "        outputs = self.decoder(target, enc_out, trg_mask)\n",
        "        output = F.softmax(self.fc_out(outputs), dim=-1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fL1EVtr4tZu8",
        "outputId": "6314745f-82b7-4989-ac68-e31a8f62af84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 12]) torch.Size([2, 12])\n",
            "Transformer(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(\n",
            "      (embed): Embedding(11, 512)\n",
            "    )\n",
            "    (positional_encoder): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (blocks): ModuleList(\n",
            "      (0): TransformerBlock(\n",
            "        (attention): MultiHeadAttention(\n",
            "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (1): TransformerBlock(\n",
            "        (attention): MultiHeadAttention(\n",
            "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (2): TransformerBlock(\n",
            "        (attention): MultiHeadAttention(\n",
            "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (3): TransformerBlock(\n",
            "        (attention): MultiHeadAttention(\n",
            "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (4): TransformerBlock(\n",
            "        (attention): MultiHeadAttention(\n",
            "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (5): TransformerBlock(\n",
            "        (attention): MultiHeadAttention(\n",
            "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(11, 512)\n",
            "    (positional_encoder): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (blocks): ModuleList(\n",
            "      (0): DecoderBlock(\n",
            "        (attention): MultiHeadAttention(\n",
            "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "        (transformerBlock): TransformerBlock(\n",
            "          (attention): MultiHeadAttention(\n",
            "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (feed_forward): Sequential(\n",
            "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): DecoderBlock(\n",
            "        (attention): MultiHeadAttention(\n",
            "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "        (transformerBlock): TransformerBlock(\n",
            "          (attention): MultiHeadAttention(\n",
            "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (feed_forward): Sequential(\n",
            "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): DecoderBlock(\n",
            "        (attention): MultiHeadAttention(\n",
            "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "        (transformerBlock): TransformerBlock(\n",
            "          (attention): MultiHeadAttention(\n",
            "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (feed_forward): Sequential(\n",
            "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): DecoderBlock(\n",
            "        (attention): MultiHeadAttention(\n",
            "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "        (transformerBlock): TransformerBlock(\n",
            "          (attention): MultiHeadAttention(\n",
            "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (feed_forward): Sequential(\n",
            "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): DecoderBlock(\n",
            "        (attention): MultiHeadAttention(\n",
            "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "        (transformerBlock): TransformerBlock(\n",
            "          (attention): MultiHeadAttention(\n",
            "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (feed_forward): Sequential(\n",
            "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): DecoderBlock(\n",
            "        (attention): MultiHeadAttention(\n",
            "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "        (transformerBlock): TransformerBlock(\n",
            "          (attention): MultiHeadAttention(\n",
            "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (feed_forward): Sequential(\n",
            "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (fc_out): Linear(in_features=512, out_features=11, bias=True)\n",
            ")\n",
            "Output Shape: torch.Size([2, 12, 11])\n"
          ]
        }
      ],
      "source": [
        "# TEST\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "src_vocab_size = 11\n",
        "target_vocab_size = 11\n",
        "num_blocks = 6\n",
        "seq_len = 512\n",
        "\n",
        "# let 0 be sos token and 1 be eos token\n",
        "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1],\n",
        "                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
        "target = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1],\n",
        "                       [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])\n",
        "\n",
        "print(src.shape, target.shape)\n",
        "model = Transformer(embed_dim=512,\n",
        "                    src_vocab_size=src_vocab_size,\n",
        "                    target_vocab_size=target_vocab_size,\n",
        "                    seq_len=seq_len,\n",
        "                    num_blocks=num_blocks,\n",
        "                    expansion_factor=4,\n",
        "                    heads=8)\n",
        "\n",
        "print(model)\n",
        "out = model(src, target)\n",
        "print(f\"Output Shape: {out.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 12])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IAKJxAjOiDi_"
      },
      "outputs": [],
      "source": [
        "a = torch.randint(3, 10, (1, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[5, 4, 4, 3, 4]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 5])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "model(a, a).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
